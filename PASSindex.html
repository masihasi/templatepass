<!doctype html>
<html>

<head>
  <title>Project Title</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <style>
    .menu-index {
      color: rgb(255, 255, 255) !important;
      opacity: 1 !important;
      font-weight: 700 !important;
    }
  </style>
</head>

<body>
  <div class="menu-container"></div>
  <div class="content-container">
    <div class="content">
      <div class="content-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
        
          
          </div>
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
      
      
      
      <div class="content">
      <div class="content-table flex-column">    

	    
	                  
  <!-- Page Content-->
<div class="container-fluid p-0">
    <!-- About-->
    <section class="resume-section" id="about">
        <div class="resume-section-content">
            <h1 class="mb-0">
                PASS
                <span class="text-info">Project (  Persian Audio Source Separation ) </span>
            </h1>
        
          
            
  
  
          
        </div>
      
	      
	    
	      
         
              
       <!-------------------------------------------------------------------------------------------------------------------------------->

  	
	
           
         
     <!-------------------------------------------------------------------------------------------->

	 <!--Start Intro-->
	  <div class="flex-row">
          <div class="flex-item flex-item-stretch flex-column">
	    <img class="image max-width-300  max-height-300 " src="imgpass/pass.jpeg">
          </div>
          <div class="flex-item flex-column">
            <p class="text">
              <a target="_blank" ></a>Internship of Asr Gooyesh Pardaz <br>
              <a target="_blank" ></a> Phone Number:</a> (+98 21) 61931000<br>
              <a target="_blank" ></a> Email: </a>nfo@asr-gooyesh.com<br>
              <a target="_blank" ></a>  Address: Unit 10, No. 2, Boroumand Alley, Teymoori Blv.,<br>
              <a target="_blank" href="javascript:void(0)"></a> Habibollah Blv., Azadi Ave, Tehran-Irany<br>
            </p>
          </div>
	     </div>     
	      
	      
 
	      
	      
	<!--End Text Only-->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Text with Buttons-->
        <div class="flex-row">
          <div class="flex-item flex-column">
        	  
            <div class="control-group">
	      <a class="custom-button-flat" href="http://asr-gooyesh.com/en/" target="_blank"><img src="img/home.png"></a>
              <a class="custom-button-flat" href="https://telegram.me/asrgooyeshpardaz" target="_blank"><img src="img/telegram.png"></a>
              <a class="custom-button-flat" href="https://www.linkedin.com/company/asr-gooyesh-pardaz-co-/?originalSubdomain=ir" target="_blank"><img src="img/linkedin.png"></a>
	      <a class="custom-button-flat" href="https://www.instagram.com/asrgooyesh/" target="_blank"><img src="img/instagram.png"></a>
              <a class="custom-button-flat" href="https://github.com/agp-internship" target="_blank"><img src="img/github.png"></a>

                
            </div>
 
          </div>
        </div>
      
      
      
              
        <!-- Page Content-->
<div class="container-fluid p-0">
    <!-- About-->
    <section class="resume-section" id="about">
        <div class="resume-section-content">
       

		
            
     <div class="lead mb-5">

        <p align="justify"> The Persian audio source separation project aims to separate two or more audio files from one audio containing two or more audio sources. 
	Consider a audio file of a meeting in which four people are talking at the same time,
	and background music is playing; the final product of this project should be able to output five separate audio files, 
	each containing one sound from one of them. Among these audio sources (4 people and a music source).
	The library in the references section is an open-source project ready for this work.
	This project may be enough for us (it may not depend on the language).
	Necessary measures should be taken to personalize it for the Persian language. After preliminary research and making a demo in the Persian language,
	it is time to test the product. It is placed as a research product in the field of sound processing. 
	It is expected that an initial report will be posted on the company's blog after the research, and the 
	community of this report and weekly reports will also make the project's final report. </p>
                
                        </div>
  
        </div>
      
      
</div>
                  <hr>

      
      
      
        
        
        
        
        
        
        
	     

    <!-- Soroush Gooran -->
    <section class="resume-section" id="Mentor">
        <div class="resume-section-content">
            <h2 class="mb-5">Mentor: Dr. Soroush Gooran</h2>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <img class="subheading mb-2" src="imgpass/DrGooran.png" style="width: 30%; padding: 5px" alt="AUT">

                    <h3 class="mb-0"> <a  style="color: black; font-size: x-large;font-weight: 500"
                                         href="">
                    </a></h3>
                    <p class="mb-2">  </p>
                    <p>     About Mentor:     PhD student in Artificial Intelligence at Sharif University of Technology  </p>
			
				                      <hr>
 

                </div>
            </div>      
        </div>
	    
	    
	    
 
	    
	    
	    
	       
  
     
    <!-- Masoumeh siar-->
    <section class="resume-section" id="Masoumehsiar">
        <div class="resume-section-content">
            <h2 class="mb-5">Masoumeh siar</h2>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <img class="subheading mb-2" src="imgpass/masoumeh.png" style="width: 30%; padding: 5px" alt="AUT">

                    <h3 class="mb-0"> <a  style="color: black; font-size: x-large;font-weight: 500"
                                         href="https://arxiv.org/pdf/1712.04555.pdf">Classification vs. Regression in Supervised Learning for Single Channel Speaker Count Estimation
                    </a></h3>

                    <div> In2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2018 Apr 15 (pp. 436-440). IEEE.</div>

			<p align="justify">  Abstract: The task of estimating the maximum number of concurrent speakers from single channel mixtures
				is important for various audio-based applications, such as blind source separation, speaker diarisation,
				audio surveillance or auditory scene classification. Building upon powerful machine learning methodology, 
				we develop a Deep Neural Network (DNN) that estimates a speaker count. While DNNs efficiently map input representations 
				to output targets, it remains unclear how to best handle the network output to infer integer source count estimates,
				as a discrete count estimate can either be tackled as a regression or a classification problem. In this paper, 
				we investigate this important design decision and also address complementary parameter choices such as the input representation. 
				We evaluate a stateof-the-art DNN audio model based on a Bi-directional Long Short-Term Memory network architecture for
				speaker count estimations. Through experimental evaluations aimed at identifying the best overall strategy for the task and 
				show results for five seconds speech segments in mixtures of up to ten speakers.</p>
                    
                </div>
            </div>

       
		
		
	 <!--Start Text with Buttons-->
        <div class="flex-row">
          <div class="flex-item flex-column">
        	  
            <div class="control-group">
	         <a class="custom-button-flat" href="https://github.com/aishoot/Concurrent_Speakers_Counter" target="_blank"><img src="img/github.png"></a>
                 <a class="custom-button-flat" href=" " target="_blank"><img src="img/telegram.png"></a>

                
            </div>
 
          </div>
        </div>   
		
		
            
        </div>
 	    
	    
       <hr>
	    
 
	     
	    
	    
	    
	    
	    
	
        
        
    <!-- Fatemeh Jafari-->
    <section class="resume-section" id="FatemehJafari">
        <div class="resume-section-content">
            <h2 class="mb-5">Fatemeh Jafari</h2>
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <img class="subheading mb-2" src="imgpass/jafari.jpg" style="width: 30%; padding: 5px" alt="AUT">

                    <h3 class="mb-0"> <a  style="color: black; font-size: x-large;font-weight: 500"
                                         href="https://arxiv.org/pdf/1805.02410.pdf">MMDENSELSTM: AN EFFICIENT COMBINATION OF CONVOLUTIONAL AND
RECURRENT NEURAL NETWORKS FOR AUDIO SOURCE SEPARATION
			    
                    </a></h3>
                    <div>  In2018 16th International workshop on acoustic signal enhancement (IWAENC) 2018 Sep 17 (pp. 106-110). IEEE.</div>
                    <p class="mb-2">  </p>
                    <p align="justify">  Abstract: Deep neural networks have become an indispensable technique for audio source separation (ASS).
			    It was recently reported that a variant of CNN architecture called MMDenseNet was successfully employed to solve the ASS 
			    problem of estimating source amplitudes, and state-of-the-art results were obtained for DSD100 dataset. To further enhance
                            MMDenseNet, here we propose a novel architecture that integrates long short-term memory (LSTM) in multiple scales with skip
			    connections to efficiently model long-term structures within an audio context. The experimental results show
                            that the proposed method outperforms MMDenseNet, LSTM and a blend of the two networks. The number of parameters 
			    and processing time of the proposed model are significantly less than those for simple blending. Furthermore,
			    the proposed method yields better results than those obtained using ideal binary masks for a singing voice separation task.</p>
                    
                </div>
            </div>

       	
	 <!--Start Text with Buttons-->
        <div class="flex-row">
          <div class="flex-item flex-column">
        	  
            <div class="control-group">
	         <a class="custom-button-flat" href="https://github.com/tsurumeso/vocal-remover" target="_blank"><img src="img/github.png"></a>
                 <a class="custom-button-flat" href=" " target="_blank"><img src="img/telegram.png"></a>

                
            </div>
 
          </div>
        </div>   
            
        </div>
 	    
	    
	    
        
        	
	    
	    
	    
	    
	    
	          
    <hr class="m-0"/>
    <!-- mohammadi-->
    <section class="resume-section" id="mohammadi">
        <div class="resume-section-content">
            <h2 class="mb-5">‪Sajjad Mohammadi and AmirAli Rezaei</h2>  
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <img class="subheading mb-2" src="imgpass/mohammadi.png" style="width: 30%; padding: 5px" alt="AUT">

                    <h3 class="mb-0"> <a  style="color: black; font-size: x-large;font-weight: 500"
                                         href="https://arxiv.org/pdf/2005.04132.pdf"> Asteroid: the PyTorch-based audio source separation toolkit for researchers

                    </a></h3>
                    <div>  arXiv preprint arXiv:2005.04132. 2020 May 8. </div>
                    <p class="mb-2">  </p>
                   <p align="justify"> Abstract: This paper describes Asteroid, the PyTorch-based audio source separation toolkit for researchers.
			   Inspired by the most successful neural source separation systems, it provides all neural building blocks required 
			   to build such a system. To improve reproducibility, Kaldi-style recipes on common audio source separation datasets
			   are also provided. This paper describes the software architecture of Asteroid and its most important features.
			   By showing experimental results obtained with Asteroid’s recipes, we show that our implementations are at least
			   on par with most results reported in reference papers. The toolkit is publicly available at github.com/mpariente/asteroid. </p>
                        
                </div>
            </div>

       	    
         <div class="flex-row">
          <div class="flex-item flex-column">
        	  
            <div class="control-group">
	         <a class="custom-button-flat" href="https://github.com/asteroid-team/asteroid" target="_blank"><img src="img/github.png"></a>
                 <a class="custom-button-flat" href=" " target="_blank"><img src="img/telegram.png"></a>

                
            </div>
 
          </div>
        </div> 
            
        </div>
 	    
	    
	    
	    
	    
	    
	    
	    
	    
       <hr class="m-0"/>
 
   
    <!-- ‪Alireza Zargaran-->
    <section class="resume-section" id="Zargaran">
        <div class="resume-section-content">
            <h2 class="mb-5">‪Alireza Zargaran</h2>  
            <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                <div class="flex-grow-1">
                    <img class="subheading mb-2" src="imgpass/Zargaran.png" style="width: 30%; padding: 5px" alt="AUT">

                    <h3 class="mb-0"> <a  style="color: black; font-size: x-large;font-weight: 500"
                                         href="https://arxiv.org/pdf/1910.09804.pdf"> TWO-STEP SOUND SOURCE SEPARATION: TRAINING ON LEARNED LATENT TARGETS
                    </a></h3>
                    <div>     InICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2020 May 4 (pp. 31-35). IEEE.
 </div>
                    <p class="mb-2">  </p>
                           Abstract: In this paper, we propose a two-step training procedure for source separation via a deep neural network.
			   In the first step we learn a transform (and it’s inverse) to a latent space where masking-based separation performance using
			   oracles is optimal. For the second step, we train a separation module that operates on the previously learned space.
			   In order to do so, we also make use of a scale-invariant signal to distortion ratio (SI-SDR) loss function that works in
			   the latent space, and we prove that it lower-bounds the SI-SDR in the time domain. We run various sound separation 
			   experiments that show how this approach can obtain better performance as compared to systems that learn the 
			   transform and the separation module jointly. The proposed methodology is general enough to be applicable to 
			   a large class of neural network end-to-end separation systems.  
                        
                </div>
            </div>

         
	      <div class="flex-row">
          <div class="flex-item flex-column">
        	  
            <div class="control-group">
	         <a class="custom-button-flat" href="https://github.com/etzinis/two_step_mask_learning" target="_blank"><img src="img/github.png"></a>
                 <a class="custom-button-flat" href=" " target="_blank"><img src="img/telegram.png"></a>

                
            </div>
 
          </div>
        </div>  
            
        </div>
  
	    
	    
	    
	    
	    
	    
	  
	    
	    
	    

	    
	    
    <hr class="m-0"/>
    <!-- Reference-->
    <section class="resume-section" id="Reference">
        <div class="resume-section-content">
            <h2 class="mb-5">Reference</h2>
            <br>
            <Ul class="colorprimary" style="font-size: larger; padding-left: 0px; letter-spacing: 0.1em">
                <li>
<p align="justify"> Stöter FR, Chakrabarty S, Edler B, Habets EA. Classification vs. regression in supervised learning for single channel speaker count estimation. In2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2018 Apr 15 (pp. 436-440). IEEE. </p>               </li>
                
                <li>
                    
              <p class="mb-3">  </p>

<p align="justify"> Takahashi N, Goswami N, Mitsufuji Y. Mmdenselstm: An efficient combination of convolutional and recurrent neural networks for audio source separation. In2018 16th International workshop on acoustic signal enhancement (IWAENC) 2018 Sep 17 (pp. 106-110). IEEE.</p>
                </li>
                
              <p class="mb-3">  </p>

                <li>
<p align="justify"> Pariente M, Cornell S, Cosentino J, Sivasankaran S, Tzinis E, Heitkaemper J, Olvera M, Stöter FR, Hu M, Martín-Doñas JM, Ditter D. Asteroid: the PyTorch-based audio source separation toolkit for researchers. arXiv preprint arXiv:2005.04132. 2020 May 8.</p>
                </li>
                
               <p class="mb-3">  </p>

                 <li>
<p align="justify"> Tzinis E, Venkataramani S, Wang Z, Subakan C, Smaragdis P. Two-step sound source separation: Training on learned latent targets. InICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2020 May 4 (pp. 31-35). IEEE. </p>             </li>
                
                <p class="mb-3">  </p>

           
             
                 </li>
                 </li>
                 </li>
     </li>
             </li>
            </Ul>
        </div>

	    
	    
	    
	    
	    
	    
	    
	    
	    
	    
	    
	    
  
	    
	    
	    
	    
	    
	    
	 
        
        
        
        
        
        
        
        
        
        
        
  
        </div>
      </div>
    </div>
  </div>
</body>

</html>
